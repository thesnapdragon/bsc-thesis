\chapter{Tesztelés és mérések}
\label{cha:test}

\section{A teszteléshez használt eszközök}
\label{sec:testtools}

A feldolgozólánc teszteléséhez elő kellett állítani először a komponensek futtatható változatát. A komponensek implementációjakor használt Bndtools eszköz ebben is nagy segítségemre volt, mert az alkalmazás futtatásakor a Bndtools automatikusan előállította a telepíthető csomagokat. Az egyes komponensek könyvtári függőségeit, amelyek nem részei az alap OSGi futtató környezetnek (tipikusan ezek a harmadik féltől származó könyvtárak) magamnak kellett előállítanom szintén a Bndtools segítségével.

A fordításnál előállított JAR formátumú bundle-ök módosítás nélkül telepíthetőek valamilyen OSGi alapú rendszert futtatni képes környezetbe (bundle repository-ba). Fejlesztés során a gyorsabb tesztelhetőség miatt az Eclipse saját OSGi implementációs megoldását az Equinox-ot használtam. Egy stabil változat elkészítése után pedig, bizonyos szinten automatizáltan, szkriptek segítségével telepítettem az alkalmazást a SZTAKI Cloud-ban foglalt virtuális gépre, melyet az alkalmazás futtatására készítettem elő tesztelés céljából.

A telepítési környezetben már a könnyen kezelhető Apache Felix \cite{apachefelix} OSGi implementációját használtam, mert annak WebConsole eszközével könnyedén kezelhetőek a komponensek menedzselése, valamint parancssoros hozzáférés is lehetséges, ahol interaktív módon menedzselhetőek a komponensek, sőt még egy egyszerűbb Bash szintaktikáját utánzó szkriptelési lehetőséget is használhatnak a fejlesztők. Teszteléskor a virtuális gépre SSH-n belépve, az Apache Felix konzolos felületét használva telepítettem az elkészített bundle-öket és függőségeiket.

\begin{figure}[htp]
\centering
\includegraphics[scale=0.4]{img/deploymentdependency}
\caption{Komponensek függőségei}
\label{fig:deploymentdependency}
\end{figure}

Telepítéshez, illetve a feldolgozólánc komponenseinek frissítéséhez az Apache Felix egy beépített megoldását az Apache Felix Gogo-t használtam, mely egy egységes, szabványos shell felületet határoz meg az OSGi alapú környezetek számára. Amint már említettem az Apache Felix Gogo-val a Unix rendszerekben ismert Bash szintaktikához hasonló parancsokat lehet használni. Ezeket a parancsokat akár szkript formájában is futtathatjuk, ez a Gogo Shell szkript, másnéven \textit{gosh} szkript.

A csomagok függőségeit ábrázolja a \ref{fig:deploymentdependency}.~ábra. A függőségi fa leveleiben található bundle-öket kell először, majd a fában a gyökér felé visszafelé haladva kell a többi bundle-t telepíteni.

A bundle-ök menedzselését a konzolos Apache Felix Gogo Shell-en kívül, egy webes felületen Apache Felix Web Console segítségével is lehet végezni. Segítségével a rendszer állapotát, tulajdonságait lehet beállítani és megfigyelni, valamint a bundle-ök részletes tulajdonságait, kapcsolatait lehet felderíteni, illetve beállításait módosítani. A webes felületről a bundle-ök telepítésével, indításával kapcsolatos funkciók szintén elérhetőek.

% section testtools (end)

\section{A feldolgozóláncra épülő kutatómodul}
\label{sec:researchbundle}

A tesztelés során egy lehetséges felhasználási módot próbáltam ki, mikor a feldolgozólánchoz egy MTA SZTAKI által készített kutatómodult illesztettem. A modul segítségével gépi tanulás, természetes nyelvi feldolgozás témakörökhöz használható program készíthető.

A kutatómodul a feldolgozólánc által készített adatbázisból szerzi meg a bemenő adatokat, ehhez a DB Connector komponens kiegészítése volt szükséges, az OSGi szolgáltatást a lekérdezésekhez szükséges metódussal kellett ellátni. A lekérdezett cikk szövegét, melyet a Parsoid parser HTML / RDFa formátumra alakított, először szintaktikailag helyes XML formátumba kell hozni (valid XHTML).

A kutatómodul a Wikipedia eltárolt cikkeiből ún. AnnotatedDocument példányokat tud készíteni, melyekben a különböző HTML elemek kategóriák szerint vannak már csoportosítva (például: linkek, paragrafusok). Az elkészített gyűjtemények alapján OpenNLP segítségével olyan természetes nyelvi feldolgozáshoz kapcsolódó feladatok végezhetőek, mint például a mondatokra bontás, szóelemzés, tokenizálás.

A kutatómodul végső felhasználása lehet például egy adott kifejezés hivatkozásainak összegyűjtése, hogy milyen kontextusban fordulnak elő a leggyakrabban, meghatározható egy nyelv átlagos szava, vagy bekezdése, rengeteg lehetőség elképzelhető.

% section researchbundle (end)

\section{Mérési eredmények}
\label{sec:measurement}

\begin{figure}[htp]
\centering
\includegraphics[scale=0.5]{img/deploymentdiagram}
\caption{A mérési összeállítás deployment diagramja}
\label{fig:deploymentdiagram}
\end{figure}

A mérési összeállítás a \ref{fig:deploymentdiagram}.~ábrán figyelhető meg. A használt virtuális gépek OpenNebula sablonjának legfontosabb paraméterei a \ref{lst:opennebulatemplates}.~kódrészletben olvashatóak. A WikiHarvester gép 4 processzort használ 16 virtuális CPU maggal és 10 Gb RAM érhető el számára. Ezen a gépen fut az OSGi keretrendszer benne a fejlesztett komponensekkel és azok függőségeivel, illetve a H2 adatbázis is itt fut külön folyamatban saját JVM-mel (a H2 adatbázissal JDBC kapcsolaton keresztül történik a kommunikáció). A WikiHarvester gép 30 darab ParsoidWorker virtuális gépet tud használni a cikkek átalakítására a Parser komponensben. A ParsoidWorker gépek 1 darab 2 magos processzort használnak fejenként, 512 Mb RAM memóriával; mindegyikre fel van telepítve a NodeJS, ahol a saját szerveroldali Parsoid megoldás fut.

\begin{lstlisting}[label={lst:opennebulatemplates}, caption=Részlet a használt VM-ek sablonjából,breaklines=true]
NAME   = [APP]WikiHarvester
CPU    = 4
VCPU   = 16
MEMORY = 10240

NAME   = [APP]ParsoidWorker
CPU    = 1
VCPU   = 2
MEMORY = 512
\end{lstlisting}

A mérés megkezdésekor letöltöttem az angol Wikipediáról egy adatbázis mentést (dump), melynek mérete 42 Gb volt. A rendszer indításakor megkezdi a feldolgozólánc a dump feldolgozását egy külön szálon, míg a főszálon on-the-fly követi a Wikipédiába kerülő cikkek változásait. A Wikipédia dump feldolgozása egy előfeldolgozó fázissal kezdődik, ahol a későbbi tényleges feldolgozófolyamat követhetőségéért először meg kell számolni, hogy hány cikket kell beilleszteni a dump-ból. 

\begin{figure}[htp]
\centering
\includegraphics[scale=0.3]{img/storedarticles}
\caption{A mérés során feldolgozott cikkek statisztikája}
\label{fig:storedarticles}
\end{figure}

A fent említett előfeldolgozás a mérés során használt Wikipédia mentéssel körülbelül ~36 percig tartott, ezalatt ~1,36 milliárd cikket olvasott be. Az on-the-fly feldolgozást végző ParsoidWorker-ek száma 25 darab volt, míg a későbbi dump feldolgozást 5 darab ParsoidWorker segítette. Az egy órás mérés végén az adatbázis mérete, üres adatbázissal indítva a mérést 644 Mb lett, ezalatt 4298 cikket dolgozott fel a rendszer. A Statistics komponens által készített részletes feldolgozási statisztikák közül egy látható a \ref{fig:storedarticles}.~ábrán.

\begin{figure}[htp]
\centering
\includegraphics[scale=0.3]{img/measurement_cpu}
\caption{A mérés során mért CPU használat}
\label{fig:measurement_cpu}
\end{figure}

A teljesítményméréseket az \textit{iostat} nevű ingyenes és nyíltforráskódú monitorozó szoftverrel végeztem, mely képes a processzor és a diszk használatot is monitorozni, valamint a különböző szálak működésének megfigyeléséhez a \textit{top} programot használtam. A processzor használatot ábrázoló diagramon (\ref{fig:measurement_cpu}.~ábra) világosan elkülönül az előfeldolgozás fázisa és az a fázis, amikor az on-the-fly és dump feldolgozás egyszerre történik. Az alkalmazás által használt processzoridő (\texttt{\%user}, mely a felhasználói módban eltöltött processzoridőt mutatja) körülbelül 2 magot használ ki a rendelkezésre álló erőforrásból, utóbbinál nyilvánvalóan visszaesik kicsit a teljesítmény és átlagosan 1 magnyi erőforrást tud kihasználni a feldolgozólánc.

\begin{figure}[htp]
\centering
\includegraphics[scale=0.3]{img/measurement_io}
\caption{A mérés során mért lemezhasználat}
\label{fig:measurement_io}
\end{figure}

Kérdés lehet, hogy egy ilyen többszálú alkalmazás, amely több szálat is használ miért nem használja ki a bőséges erőforrást? A válasz az, hogy kihasználja az erőforrásokat, csak a grafikonon ez nem figyelhető közvetlenül meg. Az alkalmazás futásakor a top programban világosan látszott, hogy a feldolgozólánc mind a 16 elérhető magot kihasználta a ThreadPool-oknak köszönhetően, azonban a program futásakor legnagyobb részben a processzor más műveletek elvégzésére vár (ezért ilyen magas az \texttt{\%idle}, azaz tétlenségi idő). Ezen időigényes műveletek hálózati kommunikációt jelentenek: a cikk szövegének letöltése Wikipedia API-n keresztül, kommunikáció a ParsoidWorker-ekkel és várakozás a visszakapott eredményekre. A processzor használat visszaesésére a második részben magyarázat még, hogy a gyorsabb és processzorigényesebb dump feldolgozást végző szálak jobban lefoglalják a processzort a többi szál elől.

A lemezhasználatot bemutató grafikon a rendszer fentiekben meghatározott két állapota még jobban meghatározható, mint a processzorhasználatnál. A lemezhasználati statisztikákból olyan metrikát választottam ki, melyből jó következtetéseket lehet levonni. Ez a metrika az átlagos várakozási idő milliszekundumban, amíg az IO művelet végződik. Látható, hogy az első fázisban viszonylag egyenletesen használja az alkalmazás a diszket, míg a dump feldolgozás fázisában hatalmas kilengések vannak és jobban meghatározza a teljesítményt a hálózati forgalomra való várakozás. Összesítve tehát látható, hogy sem a processzor, sem a diszk teljes erőforrását nem lehet kihasználni a kívánt módon implementált rendszerrel, a hálózati kommunikációra való várakozás miatt; a processzor teljesítménye nem összehasonlítható a hálózati teljesítménnyel.

% section measurement (end)

% chapter test (end)